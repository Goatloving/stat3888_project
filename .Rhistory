fold_predict<-predict(rf, fold_test)
test_error_RMSE[i]<-RMSE(fold_predict, fold_test$dia_score) # calculate and record RMSE
}
dat6 = dat3
dat6<-na.omit(dat6)
# set up k value for k-fold cross validation
k_fold=10
# set number of hiddne layers
hid_layer=5
# create k folds
folds<-createFolds(y=dat5_one_hot$dia_score, k=k_fold)
test_error_RMSE<-c()
for (i in 1:k_fold){
fold_test<-dat6_one_hot[folds[[i]],] # select folds[[i]] as test test
fold_train<-dat6_one_hot[-folds[[i]],] # remaining is training set
rf<-randomForest(dia_score ~ .-final_score-cho_score-sug_score-hyp_score, data=fold_train, ntree=100)
fold_predict<-predict(rf, fold_test)
test_error_RMSE[i]<-RMSE(fold_predict, fold_test$dia_score) # calculate and record RMSE
}
dat6 = dat3
dat6<-na.omit(dat6)
# set up k value for k-fold cross validation
k_fold=10
# set number of hiddne layers
hid_layer=5
# create k folds
folds<-createFolds(y=dat5_one_hot$dia_score, k=k_fold)
test_error_RMSE<-c()
for (i in 1:k_fold){
fold_test<-dat6[folds[[i]],] # select folds[[i]] as test test
fold_train<-dat6[-folds[[i]],] # remaining is training set
rf<-randomForest(dia_score ~ .-final_score-cho_score-sug_score-hyp_score, data=fold_train, ntree=100)
fold_predict<-predict(rf, fold_test)
test_error_RMSE[i]<-RMSE(fold_predict, fold_test$dia_score) # calculate and record RMSE
}
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE
# fit using a single tree
rp<-rpart(dia_score~.-final_score-cho_score-sug_score-hyp_score,
data = dat6,
)
plot(rp)
# fit using a single tree
rp<-rpart(dia_score~.-final_score-cho_score-sug_score-hyp_score,
data = dat6,
control=rpart.control(cp=0.0001, minsplit=5)
)
plot(rp)
fancyPlot(rp)
fancyRpartPlot(rp)
fancyRpartPlot(rp)
fancyRpartPlot(rp,palettes=c("Oranges", "Blues"), main="Waist Circumference Prediction")
library(rattle)
fancyRpartPlot(rp,palettes=c("Oranges", "Blues"), main="Waist Circumference Prediction")
fancyRpartPlot(rp,palettes=c("Oranges", "Blues"), main="Dia_score Prediction")
# fit using a single tree
rp<-rpart(dia_score~.-final_score-cho_score-sug_score-hyp_score,
data = dat6,
control=rpart.control(cp=0.0001, minsplit=5)
)
fancyRpartPlot(rp,palettes=c("Oranges", "Blues"), main="Dia_score Prediction")
fancyRpartPlot(rp,palettes=c("Oranges"), main="Dia_score Prediction")
print(table(dat6$dia_score))
library(DWmR)
install.packages('DWmR')
install.packages('smotefamily')
# Predict dia_score, drop other y and y-related variables
#dat4<-subset(dat3, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cho_score, sug_score, hyp_score, final_score))
dat4=dat3
# Predict dia_score, drop other y and y-related variables
#dat4<-subset(dat3, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cho_score, sug_score, hyp_score, final_score))
dat4=dat3
# remove NA values
dat4<-na.omit(dat4)
# set up k value for k-fold cross validation
k_fold=10
# create k folds
folds<-createFolds(y=dat4$dia_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c()
# K-fold cross-validation:
for (i in 1:k_fold){
fold_test<-dat4[folds[[i]],] # select folds[[i]] as test test
fold_train<-dat4[-folds[[i]],] # remaining is training set
# linear regression using AIC
M1<-lm(dia_score~., data=fold_train) # full model
M0<-lm(dia_score~1, data=fold_train) # null model
lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
test_error_RMSE[i] = RMSE(fold_predict, fold_test$dia_score) # calculate and record RMSE
}
# Predict dia_score, drop other y and y-related variables
dat4<-subset(dat3, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cho_score, sug_score, hyp_score, final_score))
# Predict dia_score, drop other y and y-related variables
dat4<-subset(dat3, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cho_score, sug_score, hyp_score, final_score))
#dat4=dat3
# remove NA values
dat4<-na.omit(dat4)
# set up k value for k-fold cross validation
k_fold=10
# create k folds
folds<-createFolds(y=dat4$dia_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c()
# K-fold cross-validation:
for (i in 1:k_fold){
fold_test<-dat4[folds[[i]],] # select folds[[i]] as test test
fold_train<-dat4[-folds[[i]],] # remaining is training set
# linear regression using AIC
M1<-lm(dia_score~., data=fold_train) # full model
M0<-lm(dia_score~1, data=fold_train) # null model
lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
test_error_RMSE[i] = RMSE(fold_predict, fold_test$dia_score) # calculate and record RMSE
}
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE
# Predict dia_score, drop other y and y-related variables
#dat4<-subset(dat3, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cho_score, sug_score, hyp_score, final_score))
dat4=dat3
# remove NA values
dat4<-na.omit(dat4)
# set up k value for k-fold cross validation
k_fold=10
# create k folds
folds<-createFolds(y=dat4$dia_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c()
# K-fold cross-validation:
for (i in 1:k_fold){
fold_test<-dat4[folds[[i]],] # select folds[[i]] as test test
fold_train<-dat4[-folds[[i]],] # remaining is training set
# linear regression using AIC
M1<-lm(dia_score~., data=fold_train) # full model
M0<-lm(dia_score~1, data=fold_train) # null model
lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
test_error_RMSE[i] = RMSE(fold_predict, fold_test$dia_score) # calculate and record RMSE
}
# Predict dia_score, drop other y and y-related variables
dat4<-subset(dat3, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cho_score, sug_score, hyp_score, final_score))
str(dat4)
# Predict dia_score, drop other y and y-related variables
dat4<-subset(dat3, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cho_score, sug_score, hyp_score, final_score))
str(dat4)
# remove NA values
dat4<-na.omit(dat4)
# set up k value for k-fold cross validation
k_fold=10
# create k folds
folds<-createFolds(y=dat4$dia_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c()
# K-fold cross-validation:
for (i in 1:k_fold){
fold_test<-dat4[folds[[i]],] # select folds[[i]] as test test
fold_train<-dat4[-folds[[i]],] # remaining is training set
# linear regression using AIC
M1<-lm(dia_score~., data=fold_train) # full model
M0<-lm(dia_score~1, data=fold_train) # null model
lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
test_error_RMSE[i] = RMSE(fold_predict, fold_test$dia_score) # calculate and record RMSE
}
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE
summary(lm_model)
model<-lm(dia_score~., data=dat4)
summary(model)
dat4$dia_score>0
dat44<-dat4[dat4$dia_score>0,]
str(dat44)
str(dat4)
str(dat44)
# we use dat3 as a starting point for neural network analysis
dat5=dat3
# normalize all numeric variables
mystd <-function (x){
x<- (x-mean(x, na.rm=TRUE))/sd(x, na.rm=TRUE)
}
# remove NA
dat5<-na.omit(dat5)
# normalize numeric values in dat5 such that they have mean value = 0 and std = 1
dat5_std<-dat5%>% mutate_if(is.numeric, list(mystd))
# create dataset with one hot encoding
dat5_one_hot <- one_hot(as.data.table(dat5_std))
# set up k value for k-fold cross validation
k_fold=10
# set number of hiddne layers
hid_layer=5
# create k folds
folds<-createFolds(y=dat5_one_hot$dia_score, k=k_fold)
test_error_RMSE<-c()
for (i in 1:k_fold){
fold_test<-dat5_one_hot[folds[[i]],] # select folds[[i]] as test test
fold_train<-dat5_one_hot[-folds[[i]],] # remaining is training set
network<-neuralnet(dia_score~.-final_score-cho_score-sug_score-hyp_score, data=fold_train, hidden =hid_layer)
fold_predict<-predict(network, fold_test)
test_error_RMSE[i]<-RMSE(fold_predict, fold_test$dia_score) # calculate and record RMSE
}
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE
dat6 = dat3
dat6<-na.omit(dat6)
print(table(dat6$dia_score))
library(DWmR)
dat6 = dat3
dat6<-na.omit(dat6)
print(table(dat6$dia_score))
# set up k value for k-fold cross validation
k_fold=10
# set number of hiddne layers
hid_layer=5
# create k folds
folds<-createFolds(y=dat5_one_hot$dia_score, k=k_fold)
test_error_RMSE<-c()
for (i in 1:k_fold){
fold_test<-dat6[folds[[i]],] # select folds[[i]] as test test
fold_train<-dat6[-folds[[i]],] # remaining is training set
rf<-randomForest(dia_score ~ .-final_score-cho_score-sug_score-hyp_score, data=fold_train, ntree=100)
fold_predict<-predict(rf, fold_test)
test_error_RMSE[i]<-RMSE(fold_predict, fold_test$dia_score) # calculate and record RMSE
}
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE
# fit using a single tree
rp<-rpart(dia_score~.-final_score-cho_score-sug_score-hyp_score,
data = dat6,
control=rpart.control(cp=0.0001, minsplit=5)
)
fancyRpartPlot(rp,palettes=c("Oranges"), main="Dia_score Prediction")
avg_RMSE
plot(network)
# Predict dia_score, drop other y and y-related variables
da4<-tech_biom %>% filter (AGEC >= 19, AGEC<=64, SMKSTAT==5)
# Predict dia_score, drop other y and y-related variables
da4<-tech_biom %>% filter (AGEC >= 19, AGEC<=64)
dat4<-subset(dat4, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cho_score, sug_score, hyp_score, final_score))
# Predict dia_score, drop other y and y-related variables
dat4<-subset(dat3, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cho_score, sug_score, hyp_score, final_score))
#dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64, SMKSTAT==5)  # filter age and smoke status
dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64)  # filter age and smoke status
var_list<-c("BMISC","SEX","AGEC","DIABBC","HCHOLBC","HSUGBC","HYPBC", "SYSTOL","DIASTOL","TRIGRESB","FOLATREB","CHOLRESB","LDLRESB","HBA1PREB","GLUCFREB","ALTRESB","HDLCHREB") # add/remove variables that are interested
dat2<-dat %>% select (var_list) # select columns that we are interested
str(dat2) # 3488 obs x 13 variables
dat3<-dat2 %>% mutate(
dia_score= ifelse(DIABBC==5, 0, ifelse(DIABBC==3, 1, ifelse(DIABBC==2, 2, ifelse(DIABBC==1, 3, NA)))),
cho_score= ifelse(HCHOLBC==5, 0, ifelse(HCHOLBC==3, 1, ifelse(HCHOLBC==2, 2, ifelse(HCHOLBC==1, 3, NA)))),
sug_score= ifelse(HSUGBC==5, 0, ifelse(HSUGBC==3, 1, ifelse(HSUGBC==2, 2, ifelse(HSUGBC==1, 3, NA)))),
hyp_score= ifelse(HYPBC==5, 0, ifelse(HYPBC==3, 1, ifelse(HYPBC==2, 2, ifelse(HYPBC==1, 3, NA)))),
final_score = dia_score+cho_score+sug_score+hyp_score
)
hist(dat3$final_score) # the final score is highly unbalanced
nrow(dat3[which(dat3$final_score==0),]) # the final score of 2790 observations are 0
hist(dat3$final_score) # the final score is highly unbalanced
hist(dat3$dia_score) # the final score is highly unbalanced
hist(dat3$DIABBC) # the final score is highly unbalanced
nrow(dat3[which(dat3$diabbc==0),]) # the final score of 2790 observations are 0
nrow(dat3[which(dat3$DIABBC==0),]) # the final score of 2790 observations are 0
dat3<-dat2 %>% mutate(
dia_score= ifelse(DIABBC==5, 0, ifelse(DIABBC==3, 1, ifelse(DIABBC==2, 2, ifelse(DIABBC==1, 3, NA)))),
cho_score= ifelse(HCHOLBC==5, 0, ifelse(HCHOLBC==3, 1, ifelse(HCHOLBC==2, 2, ifelse(HCHOLBC==1, 3, NA)))),
sug_score= ifelse(HSUGBC==5, 0, ifelse(HSUGBC==3, 1, ifelse(HSUGBC==2, 2, ifelse(HSUGBC==1, 3, NA)))),
hyp_score= ifelse(HYPBC==5, 0, ifelse(HYPBC==3, 1, ifelse(HYPBC==2, 2, ifelse(HYPBC==1, 3, NA)))),
final_score = dia_score+cho_score+sug_score+hyp_score
)
nrow(dat3[which(dat3$DIABBC==0),]) # the final score of 2790 observations are 0
dat3
#dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64, SMKSTAT==5)  # filter age and smoke status
dat<-tech_biom %>% filter (AGEC >= 19, AGEC<=64)  # filter age and smoke status
var_list<-c("BMISC","SEX","AGEC","DIABBC","HCHOLBC","HSUGBC","HYPBC", "SYSTOL","DIASTOL","TRIGRESB","FOLATREB","CHOLRESB","LDLRESB","HBA1PREB","GLUCFREB","ALTRESB","HDLCHREB") # add/remove variables that are interested
dat2<-dat %>% select (var_list) # select columns that we are interested
dat3<-dat2 %>% mutate(
dia_score= ifelse(DIABBC==5, 0, ifelse(DIABBC==3, 1, ifelse(DIABBC==2, 2, ifelse(DIABBC==1, 3, NA)))),
cho_score= ifelse(HCHOLBC==5, 0, ifelse(HCHOLBC==3, 1, ifelse(HCHOLBC==2, 2, ifelse(HCHOLBC==1, 3, NA)))),
sug_score= ifelse(HSUGBC==5, 0, ifelse(HSUGBC==3, 1, ifelse(HSUGBC==2, 2, ifelse(HSUGBC==1, 3, NA)))),
hyp_score= ifelse(HYPBC==5, 0, ifelse(HYPBC==3, 1, ifelse(HYPBC==2, 2, ifelse(HYPBC==1, 3, NA)))),
final_score = dia_score+cho_score+sug_score+hyp_score
)
nrow(dat3[which(dat3$DIABBC==0),]) # the final score of 2790 observations are 0
dat3
nrow(dat3[which(dat3$DIABBC==5),]) # the final score of 2790 observations are 0
nrow(dat3[which(dat3$DIABBC==1),]) # the final score of 2790 observations are 0
nrow(dat3[which(dat3$DIABBC==3),]) # the final score of 2790 observations are 0
nrow(dat3[which(dat3$DIABBC==5),]) # the final score of 2790 observations are 0
skimr(dat2$DIABBC)
install.packages('skimr')
library(skimr)
skimr(dat2$DIABBC)
skim(dat2$DIABBC)
hist(dat3$dia_score) # the final score is highly unbalanced
skim(dat3)
dat3<-dat2 %>% mutate(
dia_score= ifelse(DIABBC==5, 0, ifelse(DIABBC==3, 100, ifelse(DIABBC==2, 200, ifelse(DIABBC==1, 300, NA)))),
cho_score= ifelse(HCHOLBC==5, 0, ifelse(HCHOLBC==3, 1, ifelse(HCHOLBC==2, 2, ifelse(HCHOLBC==1, 3, NA)))),
sug_score= ifelse(HSUGBC==5, 0, ifelse(HSUGBC==3, 1, ifelse(HSUGBC==2, 2, ifelse(HSUGBC==1, 3, NA)))),
hyp_score= ifelse(HYPBC==5, 0, ifelse(HYPBC==3, 1, ifelse(HYPBC==2, 2, ifelse(HYPBC==1, 3, NA)))),
final_score = dia_score+cho_score+sug_score+hyp_score
)
# Predict dia_score, drop other y and y-related variables
dat4<-subset(dat3, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cho_score, sug_score, hyp_score, final_score))
# Predict dia_score, drop other y and y-related variables
dat4<-subset(dat3, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cho_score, sug_score, hyp_score, final_score))
# str(dat4)
# dat44<-dat4[dat4$dia_score>0,]
# str(dat44)
# remove NA values
dat4<-na.omit(dat4)
# set up k value for k-fold cross validation
k_fold=10
# create k folds
folds<-createFolds(y=dat4$dia_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c()
# K-fold cross-validation:
for (i in 1:k_fold){
fold_test<-dat4[folds[[i]],] # select folds[[i]] as test test
fold_train<-dat4[-folds[[i]],] # remaining is training set
# linear regression using AIC
M1<-lm(dia_score~., data=fold_train) # full model
M0<-lm(dia_score~1, data=fold_train) # null model
lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
test_error_RMSE[i] = RMSE(fold_predict, fold_test$dia_score) # calculate and record RMSE
}
#summary(lm_model)
#model<-lm(dia_score~., data=dat4)
#summary(model)
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE
#summary(lm_model)
#model<-lm(dia_score~., data=dat4)
summary(model)
summary(lm_model)
hist(dat3$dia_score) # the final score is highly unbalanced
summary(lm_model)
dat3<-dat2 %>% mutate(
dia_score= ifelse(DIABBC==5, 0, ifelse(DIABBC==3, 1, ifelse(DIABBC==2, 2, ifelse(DIABBC==1, 3, NA)))),
cho_score= ifelse(HCHOLBC==5, 0, ifelse(HCHOLBC==3, 1, ifelse(HCHOLBC==2, 2, ifelse(HCHOLBC==1, 3, NA)))),
sug_score= ifelse(HSUGBC==5, 0, ifelse(HSUGBC==3, 1, ifelse(HSUGBC==2, 2, ifelse(HSUGBC==1, 3, NA)))),
hyp_score= ifelse(HYPBC==5, 0, ifelse(HYPBC==3, 1, ifelse(HYPBC==2, 2, ifelse(HYPBC==1, 3, NA)))),
final_score = dia_score+cho_score+sug_score+hyp_score
)
# Predict dia_score, drop other y and y-related variables
dat4<-subset(dat3, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cho_score, sug_score, hyp_score, final_score))
# Predict dia_score, drop other y and y-related variables
dat4<-subset(dat3, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cho_score, sug_score, hyp_score, final_score))
# str(dat4)
# dat44<-dat4[dat4$dia_score>0,]
# str(dat44)
# remove NA values
dat4<-na.omit(dat4)
# set up k value for k-fold cross validation
k_fold=10
# create k folds
folds<-createFolds(y=dat4$dia_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c()
# K-fold cross-validation:
for (i in 1:k_fold){
fold_test<-dat4[folds[[i]],] # select folds[[i]] as test test
fold_train<-dat4[-folds[[i]],] # remaining is training set
# linear regression using AIC
M1<-lm(dia_score~., data=fold_train) # full model
M0<-lm(dia_score~1, data=fold_train) # null model
lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
test_error_RMSE[i] = RMSE(fold_predict, fold_test$dia_score) # calculate and record RMSE
}
summary(lm_model)
#model<-lm(dia_score~., data=dat4)
#summary(model)
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE
summary(lm_model)
set.seed(2021)
set.seed(2021)
# Predict dia_score, drop other y and y-related variables
dat4<-subset(dat3, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cho_score, sug_score, hyp_score, final_score))
# str(dat4)
# dat44<-dat4[dat4$dia_score>0,]
# str(dat44)
# remove NA values
dat4<-na.omit(dat4)
# set up k value for k-fold cross validation
k_fold=10
# create k folds
folds<-createFolds(y=dat4$dia_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c()
# K-fold cross-validation:
for (i in 1:k_fold){
fold_test<-dat4[folds[[i]],] # select folds[[i]] as test test
fold_train<-dat4[-folds[[i]],] # remaining is training set
# linear regression using AIC
M1<-lm(dia_score~., data=fold_train) # full model
M0<-lm(dia_score~1, data=fold_train) # null model
lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
test_error_RMSE[i] = RMSE(fold_predict, fold_test$dia_score) # calculate and record RMSE
}
summary(lm_model)
#model<-lm(dia_score~., data=dat4)
#summary(model)
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE
summary(lm_model)
dat3<-dat2 %>% mutate(
dia_score= ifelse(DIABBC==5, 0, ifelse(DIABBC==3, 100, ifelse(DIABBC==2, 200, ifelse(DIABBC==1, 300, NA)))),
cho_score= ifelse(HCHOLBC==5, 0, ifelse(HCHOLBC==3, 1, ifelse(HCHOLBC==2, 2, ifelse(HCHOLBC==1, 3, NA)))),
sug_score= ifelse(HSUGBC==5, 0, ifelse(HSUGBC==3, 1, ifelse(HSUGBC==2, 2, ifelse(HSUGBC==1, 3, NA)))),
hyp_score= ifelse(HYPBC==5, 0, ifelse(HYPBC==3, 1, ifelse(HYPBC==2, 2, ifelse(HYPBC==1, 3, NA)))),
final_score = dia_score+cho_score+sug_score+hyp_score
)
set.seed(2021)
# Predict dia_score, drop other y and y-related variables
dat4<-subset(dat3, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cho_score, sug_score, hyp_score, final_score))
# str(dat4)
# dat44<-dat4[dat4$dia_score>0,]
# str(dat44)
# remove NA values
dat4<-na.omit(dat4)
# set up k value for k-fold cross validation
k_fold=10
# create k folds
folds<-createFolds(y=dat4$dia_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c()
# K-fold cross-validation:
for (i in 1:k_fold){
fold_test<-dat4[folds[[i]],] # select folds[[i]] as test test
fold_train<-dat4[-folds[[i]],] # remaining is training set
# linear regression using AIC
M1<-lm(dia_score~., data=fold_train) # full model
M0<-lm(dia_score~1, data=fold_train) # null model
lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
test_error_RMSE[i] = RMSE(fold_predict, fold_test$dia_score) # calculate and record RMSE
}
summary(lm_model)
#model<-lm(dia_score~., data=dat4)
#summary(model)
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE
summary(lm_model)
dat3<-dat2 %>% mutate(
dia_score= ifelse(DIABBC==5, 0, ifelse(DIABBC==3, 1, ifelse(DIABBC==2, 2, ifelse(DIABBC==1, 3, NA)))),
cho_score= ifelse(HCHOLBC==5, 0, ifelse(HCHOLBC==3, 1, ifelse(HCHOLBC==2, 2, ifelse(HCHOLBC==1, 3, NA)))),
sug_score= ifelse(HSUGBC==5, 0, ifelse(HSUGBC==3, 1, ifelse(HSUGBC==2, 2, ifelse(HSUGBC==1, 3, NA)))),
hyp_score= ifelse(HYPBC==5, 0, ifelse(HYPBC==3, 1, ifelse(HYPBC==2, 2, ifelse(HYPBC==1, 3, NA)))),
final_score = dia_score+cho_score+sug_score+hyp_score
)
set.seed(2021)
# Predict dia_score, drop other y and y-related variables
dat4<-subset(dat3, select=-c(DIABBC, HCHOLBC, HSUGBC, HYPBC, cho_score, sug_score, hyp_score, final_score))
# str(dat4)
# dat44<-dat4[dat4$dia_score>0,]
# str(dat44)
# remove NA values
dat4<-na.omit(dat4)
# set up k value for k-fold cross validation
k_fold=10
# create k folds
folds<-createFolds(y=dat4$dia_score, k=k_fold)
# create a new vector to record test results
test_error_RMSE<-c()
# K-fold cross-validation:
for (i in 1:k_fold){
fold_test<-dat4[folds[[i]],] # select folds[[i]] as test test
fold_train<-dat4[-folds[[i]],] # remaining is training set
# linear regression using AIC
M1<-lm(dia_score~., data=fold_train) # full model
M0<-lm(dia_score~1, data=fold_train) # null model
lm_model<-step(M1, scope=list(lower=M0, upper=M1),direction='backward', k=2) # backward selection from full model to null model
fold_predict<-predict(lm_model, type='response', newdata=fold_test) # predict y
test_error_RMSE[i] = RMSE(fold_predict, fold_test$dia_score) # calculate and record RMSE
}
summary(lm_model)
#model<-lm(dia_score~., data=dat4)
#summary(model)
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE
summary(lm_model)
#model<-lm(dia_score~., data=dat4)
#summary(model)
avg_RMSE<-sum(test_error_RMSE)/k_fold
avg_RMSE
summary(lm_model)
install.packages('DMwR')
library(DMwR)
install.packages('DMwR')
install.packages('smotefamily')
library(smotefamily)
library(tidyverse)
#library(here)      # directory referencing
#library(readxl)    # reading Excel files
#library(janitor)   # data cleaning
#library(stringr)   # string manipulation
library(tidyr)     # new tidy functions
library(knitr) # kable
#library(modi) # ok for multivariate outlier detection
library(caret)# low variance filter
install.packages('caret')
library(tidyverse)
#library(here)      # directory referencing
#library(readxl)    # reading Excel files
#library(janitor)   # data cleaning
#library(stringr)   # string manipulation
library(tidyr)     # new tidy functions
library(knitr) # kable
#library(modi) # ok for multivariate outlier detection
library(caret)# low variance filter
